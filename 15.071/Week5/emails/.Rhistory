length(wiki$Vandal==1)
str(wiki$Vandal)
summary(wiki)
table(wiki$Vandal)
View(wiki)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, removeWords, c(stopwords("english")))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, sparse = 0.97)
sparseAdded
sparseAdded = removeSparseTerms(dtmAdded, sparse = 0.90)
sparseAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.90)
sparseAdded
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.90)
sparseAdded
inspect(dtmAdded[2000:2005],[4000:4010])
inspect(dtmAdded[2000:2005, 4000:4010])
View(wiki)
inspect(dtmAdded[2000:2010, 4000:4010])
sparseAdded = removeSparseTerms(dtmAdded, 0.3)
sparseAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.0)
sparseAdded = removeSparseTerms(dtmAdded, 0.01)
sparseAdded
setwd("/home/musigma/MOOC/15.071/Week5/wiki/")
rm(list=ls())
library(tm)
library(SnowballC)
wiki <- read.csv('wiki.csv', stringsAsFactors = F)
wiki$Vandal <- as.factor(wiki$Vandal)
# cases of vandalism
table(wiki$Vandal)
# corpus for added words
corpusAdded = Corpus(VectorSource(wiki$Added))
# Stopwords
corpusAdded = tm_map(corpusAdded, removeWords, c(stopwords("english")))
# stemming the words
corpusAdded = tm_map(corpusAdded, stemDocument)
# DocumentTermMatrix
dtmAdded = DocumentTermMatrix(corpusAdded)
# Terms-6675
inspect(dtmAdded[2000:2010, 4000:4010])
findFreqTerms(dtmAdded, lowfreq = 20)
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
corpusRemoved = Corpus(VectorSource(wiki$Removed))
# Stopwords
corpusRemoved = tm_map(corpusRemoved, removeWords, c(stopwords("english")))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
dtmRemoved
sparseRemoved = removeSparseTerms(dtmRemoved, sparse = 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
View(wordsAdded)
ncol(wordsRemoved)
# Combine the two data frames into a data frame
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal <- wiki$Vandal
set.seed(123)
sample.split(wikiWords$Vandal, SplitRatio = 0.7)
set.seed(123)
split = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
trainWiki = subset(wikiWords, split = TRUE)
testWiki = subset(wikiWords, split = FALSE)
table(testWiki)
table(testWiki$Vandal)
1815/(1815+2061)
2061/(1815+2061)
wikiCART <- rpart(Vandal~., data = trainWiki, method = "class")
prp(wikiCART)
predictCART = predict(wikiCART, newdata = testWiki)
table(testWiki$Vandal, predictCART)
length(pred)
table(testWiki$Vandal, predictCART)
length(predictCART)
length(testWiki$Vandal)
prp(wikiCART)
View(predictCART)
predictCART = predict(wikiCART, newdata = testWiki, type = "class")
table(testWiki$Vandal, predictCART)
(2061+45)/(2061+45+1+1770)
table(testWiki$Vandal)
source('~/MOOC/15.071/Week5/wiki/wiki.R', echo=TRUE)
2061/(2061+1815)
wikiWords2 = wikiWords
View(wikiWords)
View(wiki)
wikiWords2$HTTP = ifelse(grepl("http", wiki$Added, fixed = TRUE), 1, 0)
table(wikiWords2$HTTP)
wikiTrain2 = subset(wikiWords2, split==TRUE)
wikiTest2 = subset(wikiWords2, split==FALSE)
wiki2CART <- rpart(Vandal~., data = wikiTrain2, method = "class")
prp(wiki2CART)
#Accuracy
predict2CART = predict(wiki2CART, newdata = wikiTest2, type = "class")
table(wikiTest2$Vandal, predict2CART)
(609+57)/(609+57+9+488)
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
View(wikiWords2)
mean(wikiWords2$NumWordsAdded)
wikiTrain2 = subset(wikiWords2, split==TRUE)
wikiTest2 = subset(wikiWords2, split==FALSE)
# new CART
wiki3CART <- rpart(Vandal~., data = wikiTrain2, method = "class")
prp(wiki3CART)
#Accuracy
predict3CART = predict(wiki3CART, newdata = wikiTest2, type = "class")
table(wikiTest2$Vandal, predict3CART)
(514+248)/(514+248+104+297)
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
# Recreate CART model
wikiTrain3 = subset(wikiWords3, split==TRUE)
wikiTest3 = subset(wikiWords3, split==FALSE)
# new CART
wiki3CART <- rpart(Vandal~., data = wikiTrain3, method = "class")
prp(wiki3CART)
#Accuracy
predict3CART = predict(wiki3CART, newdata = wikiTest3, type = "class")
table(wikiTest3$Vandal, predict3CART)
(595+241)/(595+241+23+304)
rm(list=ls())
setwd("/home/musigma/MOOC/15.071/Week5/AUTOMATING REVIEWS IN MEDICINE/")
trials <- read.csv('clinical_trial.csv', stringsAsFactors = FALSE)
summary(trails)
summary(trials)
View(trials)
str(trials)
max(nchar(trials$abstract))
summary(trials)
max(nchar(trials$abstract))
summary(nchar(trials$abstract))
table(nchar(trials$abstract))
table(nchar(trials$abstract) == 0)
trials$title[nchar(trials$title) == min(nchar(trials$title))]
library(tm)
corpusTitle = Corpus(VectorSource(trials$trial))
corpusAbstract = Corpus(VectorSource(trials$abstract))
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, c(stopwords("english")))
corpusAbstract = tm_map(corpusAbstract, removeWords, c(stopwords("english")))
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmTitle
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmTitle
rm(list=ls())
setwd("/home/musigma/MOOC/15.071/Week5/AUTOMATING REVIEWS IN MEDICINE/")
library(tm)
trials <- read.csv('clinical_trial.csv', stringsAsFactors = FALSE)
rm(list=ls())
setwd("/home/musigma/MOOC/15.071/Week5/clinical trial/")
library(tm)
trials <- read.csv('clinical_trial.csv', stringsAsFactors = FALSE)
corpusTitle = Corpus(VectorSource(trials$trial))
corpusAbstract = Corpus(VectorSource(trials$abstract))
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
# Punctuation
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
# remove stop words
corpusTitle = tm_map(corpusTitle, removeWords, c(stopwords("english")))
corpusAbstract = tm_map(corpusAbstract, removeWords, c(stopwords("english")))
# stem words
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
#Document term matrix
dtmTitle
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract
dtmTitle
rm(list=ls())
setwd("/home/musigma/MOOC/15.071/Week5/clinical trial/")
library(tm)
trials <- read.csv('clinical_trial.csv', stringsAsFactors = FALSE)
summary(trials)
corpusTitle = Corpus(VectorSource(trials$trial))
corpusAbstract = Corpus(VectorSource(trials$abstract))
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, c(stopwords("english")))
corpusAbstract = tm_map(corpusAbstract, removeWords, c(stopwords("english")))
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
#Document term matrix
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmAbstract
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
View(dtmTitle)
rm(list=ls())
setwd("/home/musigma/MOOC/15.071/Week5/clinical trial/")
library(tm)
trials <- read.csv('clinical_trial.csv', stringsAsFactors = FALSE)
summary(trials)
# #characters are there in the longest abstract
max(nchar(trials$abstract))
table(nchar(trials$abstract) == 0)
trials$title[nchar(trials$title) == min(nchar(trials$title))]
# A decade of letrozole: FACE.
# to corpus
rm(list=ls())
setwd("/home/musigma/MOOC/15.071/Week5/clinical trial/")
library(tm)
trials <- read.csv('clinical_trial.csv', stringsAsFactors = FALSE)
summary(trials)
# #characters are there in the longest abstract
max(nchar(trials$abstract))
table(nchar(trials$abstract) == 0)
trials$title[nchar(trials$title) == min(nchar(trials$title))]
# A decade of letrozole: FACE.
# to corpus
corpusTitle = Corpus(VectorSource(trials$title))
corpusAbstract = Corpus(VectorSource(trials$abstract))
# to lower case
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
# Punctuation
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
# remove stop words
corpusTitle = tm_map(corpusTitle, removeWords, c(stopwords("english")))
corpusAbstract = tm_map(corpusAbstract, removeWords, c(stopwords("english")))
# stem words
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
#Document term matrix
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
# limit to terms with sparseness of at most 95%
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
# convert to dataframe
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
colSums(dtmAbstract)
which(colSums(dtmAbstract) == max(colSums(dtmAbstract)))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
dtm = cbind(dtmTitle, dtmAbstract)
dtm$trial = trials$trial
ncol(dtm)
library(caTools)
set.seed(144)
spl = sample.split(dtm$trial, SplitRatio = 0.7)
trialTrain = subset(dtm, spl = TRUE)
trialTest = subset(dtm, spl = FALSE)
table(trialTrain)
View(trialTrain)
table(trialTrain$trial)
1043/nrow(trialTrain)
trialCART = rpart(trial~., data = trialTrain, method = 'class')
prp(trialCART)
max(predict(trialCART))
(predict(trialCART))
(predict(trialCART))[0:2]
(predict(trialCART)).head()
head(predict(trialCART))
head(predict(trialCART)[1])
head(predict(trialCART)[,1])
head(predict(trialCART)[,2])
maxe(predict(trialCART)[,2])
max(predict(trialCART)[,2])
max(predict(trialCART)[,1])
max(predict(trialCART, newdata = trialTrain, method = "class")
)
max(predict(trialCART, newdata = trialTrain, method = "class")[,2])
max(predict(trialCART)[,2])
predicTrain = predict(trialCART, type = 'class')
table(trialTrain, predicTrain)
predicTrain
table(trialTrain$trial, predicTrain)
(870+655)/(nrow(trialTrain))
655/(655+162)
870/(870+173)
441/(131+441)
631/(631+99)
rm(list=ls())
setwd("/home/musigma/MOOC/15.071/Week5/clinical trial/")
library(tm)
trials <- read.csv('clinical_trial.csv', stringsAsFactors = FALSE)
summary(trials)
# #characters are there in the longest abstract
max(nchar(trials$abstract))
table(nchar(trials$abstract) == 0)
trials$title[nchar(trials$title) == min(nchar(trials$title))]
# A decade of letrozole: FACE.
# to corpus
corpusTitle = Corpus(VectorSource(trials$title))
corpusAbstract = Corpus(VectorSource(trials$abstract))
# to lower case
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
# Punctuation
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
# remove stop words
corpusTitle = tm_map(corpusTitle, removeWords, c(stopwords("english")))
corpusAbstract = tm_map(corpusAbstract, removeWords, c(stopwords("english")))
# stem words
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
#Document term matrix
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
# limit to terms with sparseness of at most 95%
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
# convert to dataframe
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
# most frequent word
which(colSums(dtmAbstract) == max(colSums(dtmAbstract)))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
dtm = cbind(dtmTitle, dtmAbstract)
dtm$trial = trials$trial
library(caTools)
set.seed(144)
spl = sample.split(dtm$trial, SplitRatio = 0.7)
trialTrain = subset(dtm, spl = TRUE)
trialTest = subset(dtm, spl = FALSE)
trialCART = rpart(trial~., data = trialTrain, method = 'class')
predicTrain = predict(trialCART, type = 'class')
table(trialTrain$trial, predicTrain)
predTest = predict(trialCART, newdata = trialTest, method = 'class')
table(trialTest$trial, predTest)
predTest
predTest = predict(trialCART, newdata = trialTest, type = 'class')
table(trialTest$trial, predTest)
(870+655+)/(870+655+173+162)
(870+655)/(870+655+173+162)
(293+21)/(293+7+34+21)
(293+21)/(293+7+34+21)
set.seed(144)
spl = sample.split(dtm$trial, SplitRatio = 0.7)
trialTrain = subset(dtm, spl = TRUE)
set.seed(144)
spl = sample.split(dtm$trial, SplitRatio = 0.7)
trialTrain = subset(dtm, spl == TRUE)
trialTest = subset(dtm, spl == FALSE)
trialCART = rpart(trial~., data = trialTrain, method = 'class')
prp(trialCART)
predicTrain = predict(trialCART, type = 'class')
table(trialTrain$trial, predicTrain)
(631+441)/(nrow(trialTrain))
631/(631+99)
441/(441+131)
predTest = predict(trialCART, newdata = trialTest, type = 'class')
table(trialTest$trial, predTest)
(261+162)/(261+162+52+83)
library(ROCR)
predTest = predict(trialCART, newdata = trialTest)
predROCR = prediction(predTest[,2], trialTest$trial)
auc = as.numeric(performance(predROCR, "auc")@y.values)
auc
rm(list=ls())
setwd("/home/musigma/MOOC/15.071/Week5/spam/")
emails = read.csv('emails.csv', stringsAsFactors = FALSE)
View(emails)
View(emails)
str(emails)
summary(emails)
table(emails$spam)
View(emails)
max(nchar(emails$text))
which(nchar(emails$text) == min(nchar(emails$text)))
corpus = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("english")))
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, PlainTextDocument)
dtm = DocumentTermMatrix(corpus)
dtm
rm(list=ls())
setwd("/home/musigma/MOOC/15.071/Week5/emails/")
emails = read.csv('emails.csv', stringsAsFactors = FALSE)
table(emails$spam)
corpus = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, c(stopwords("english")))
corpus = tm_map(corpus, stemDocument)
corpus = tm_map(corpus, PlainTextDocument)
dtm = DocumentTermMatrix(corpus)
dtm
library(tm)
library(SnowballC)
corpus = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, removePunctuation)
corpus = tm_map(corpus, removeWords, stopwords("english"))
corpus = tm_map(corpus, stemDocument)
dtm = DocumentTermMatrix(corpus)
dtm
spdtm = removeSparseTerms(dtm, sparse = 0.95)
spdtm
emailsSparse <- as.data.frame(as.matrix(spdtm))
colnames(emailsSparse) = make.names(colnames(emailsSparse))
which.max(colSums(emailsSparse))
emailsSparse$spam = emails$spam
colSums(emailsSparse[which(emailsSparse$spam == 0),])
colSums(emailsSparse[which(emailsSparse$spam == 0),]) > 5000
sum(colSums(emailsSparse[which(emailsSparse$spam == 0),]) > 5000)
sort(colSums(subset(emailsSparse, spam == 0)))
sum(colSums(emailsSparse[which(emailsSparse$spam == 0),]) > 1000)
sum(colSums(emailsSparse[which(emailsSparse$spam == 0),]) >= 1000)
sum(colSums(subset(emailsSparse, spam == 1)) >= 1000)
sum(colSums(subset(emailsSparse, spam == 1)) > 1000)
sort(colSums(subset(emailsSparse, spam == 1)) > 1000)
sort(colSums(subset(emailsSparse, spam == 1)))
emailsSparse$spam = as.factor(emailsSparse$spam)
library(caTools)
set.seed(123)
spl = sample.split(emailsSparse$spam, SplitRatio = 0.7)
train = subset(emailsSparse, subset == TRUE)
train = subset(emailsSparse, spl == TRUE)
test = subset(emailsSparse, spl == FALSE)
spamLog = glm(spam~., data = train, family = "binomial")
spamCART = rpart(spam~., data = train, method = "class")
spamRF = randomForest(spam~., data = train)
set.seed(123)
spamRF = randomForest(spam~., data = train)
predictLog = predict(spamLog, type = "response")
predictCART = predict(spamCART)
predictRF = predict(spamRF, type = "prob")
predictLog
spamLog = glm(spam~., data = train, family = "binomial")
sum(predictLog<0.00001)
sum(predictLog<0.99999)
sum(predictLog>0.99999)
sum(predictLog<=0.99999 & predictLog >= 0.00001)
nrow(train)
3046+954+10
summary(spamLog)
prp(spamCART)
table(train$spam, predictLog>0.5)
(nrow(train)-4)/(nrow(train))
rocr = as.numeric(performance(prediction(predictLog, train$spam), "auc")@y.values))
rocr = as.numeric(performance(prediction(predictLog, train$spam), "auc")@y.values)
rocr
table(train$spam, predictCART)
table(train$spam, predictCART[,2]>0.5)
(2885+894+167+64)/(2885+894+64+167)
(2885+894)/(2885+894+64+167)
rocrTrainLog = as.numeric(performance(prediction(predictLog, train$spam), "auc")@y.values)
rocrTrainCART = as.numeric(performance(prediction(predictCART, train$spam), "auc")@y.values)
rocrTrainCART = as.numeric(performance(prediction(predictCART[,2], train$spam), "auc")@y.values)
rocrTrainCART
predictRF
table(train$spam, predictRF[,2]>0.5)
(3013+914)/(3013+914+44+39)
rocrTrainRF = as.numeric(performance(prediction(predictRF[,2], rain$spam), "auc")@y.values)
rocrTrainRF = as.numeric(performance(prediction(predictRF[,2], train$spam), "auc")@y.values)
rocrTrainRF
(3013+914)/(3013+914+44+39)
(2885+894)/(2885+894+64+167)
(nrow(train)-4)/(nrow(train))
predictLogTest = predict(spamLog, newdata = test)
table(test$spam, predictLogTest)
table(test$spam, predictLogTest > 0,5)
table(test$spam, predictLogTest > 0.5)
(1258+376)/nrow(test)
as.numeric(performance(prediction(predictLogTest, test$spam))@y.values)
as.numeric(performance(prediction(predictLogTest, test$spam), "auc")@y.values)
predictCARTTest = predict(spamCART, newdata = test)
table(test$spam, predictCARTTest[,2]>0.5)
(1228+386)/nrow(test)
as.numeric(performance(prediction(predictCARTTest[,2], test$spam), "auc")@y.values)
predictRFTest = predict(spamRF, newdata = test)
table(test$spam, predictRFTest[,2]>0.5)
table(test$spam, predictRFTest>0.5)
predictRFTest = predict(spamRF, newdata = test, type = "prob")
table(test$spam, predictRFTest>0.5)
table(test$spam, predictRFTest[,2]>0.5)
(1290+385)/nrow(test)
as.numeric(performance(prediction(predictRFTest[,2], test$spam), "auc")@y.values)
